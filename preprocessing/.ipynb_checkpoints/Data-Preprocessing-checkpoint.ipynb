{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- 构造feature之前的数据预处理\n",
    "    - 有时数据会分散在几个不同的文件中，需要 Join 起来。\n",
    "    - 处理 Missing Data。\n",
    "    - 处理 Outlier。\n",
    "    - 必要时转换某些 Categorical Variable 的表示方式。\n",
    "    - 有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 Noise，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。\n",
    "\n",
    "这一部分的处理策略多半依赖于在前一步中探索数据集所得到的结论以及创建的可视化图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# First, we'll import pandas, a data processing and CSV file I/O library\n",
    "import pandas as pd\n",
    "\n",
    "# We'll also import seaborn, a Python graphing library\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "# Next, we'll load the Iris flower dataset, which is in the \"../input/\" directory\n",
    "iris = pd.read_csv(\"../input/Iris.csv\") # the iris dataset is now a Pandas DataFrame\n",
    "\n",
    "# Let's see what's in the iris data - Jupyter notebooks print the result of the last thing you do\n",
    "iris.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Species          150 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# 查看每个特征数据量情况\n",
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值较多的特征处理\n",
    "一般如果特征的缺失量过大，会直接将该特征舍弃掉，否则可能反倒会带入较大的noise，这里先用两类分类\n",
    "```\n",
    "def set_salary_change(df):\n",
    "    df,loc[(df.salary_change.notnull()), 'salary_change' = \"Yes\"]\n",
    "    df,loc[(df.salary_change.isnull()), 'salary_change' = \"No\"]\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值较少的特征处理\n",
    "#### 如特征缺失值在10%以内，可以采取以下方式处理：\n",
    "- 把NaN直接作为一个特征，假设用0表示，实现如下：\n",
    "```\n",
    "data_train.fillna(0)\n",
    "```\n",
    "\n",
    "- 用均值填充：\n",
    "```\n",
    "# 所有行用各自的均值填充\n",
    "data_train.fillna(data_train.mean())\n",
    "# 指定某些列填充\n",
    "data——train.fillna(data_train.mean()['browse_his':'card_num'])\n",
    "```\n",
    "如果训练集train中有缺失值，而test中无缺失值，应该对缺失值取条件中值或者条件均值，根据用户label值类别取所有该label下用户该属性的均值或中值\n",
    "\n",
    "- 用上下数据进行填充\n",
    "```\n",
    "data_train.fillna(method='pad')\n",
    "data_train.fillna(method='bfill')\n",
    "```\n",
    "\n",
    "- 用插值法填充\n",
    "```\n",
    "# 插值法就是用（x0, y0）,(x1, y1)估计中间点的值\n",
    "interpolate()\n",
    "```\n",
    "\n",
    "- 用算法拟合进行填充\n",
    "```\n",
    "def set_missing_browse_his(df):\n",
    "    # 把已有的数值型特征取出来输入到RandomForestRegressor中\n",
    "    process_df = df[['browse_his', 'gender', 'job', 'edu', 'marriage', 'family_type']]\n",
    "    \n",
    "    # 分为已知该特征的和未知该特征的，两部分\n",
    "    known = process_df[process_df.browse_his.notnull()].as_matrix()\n",
    "    unknown = process_df[process_df.browse_his.isnull()].as_matrix()\n",
    "    \n",
    "    # X为特征属性值\n",
    "    X = known[:, 1:]\n",
    "    \n",
    "    # y为结果标签值\n",
    "    y = known[:, 0]\n",
    "    \n",
    "    # fit到RandomForestRegressor中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimator=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行位置特征值的预测\n",
    "    predicted = rfr.predict(unknown[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[df.browse_his.isnull(), 'browse_his'] = predicted\n",
    "    \n",
    "    return df, rfr \n",
    "```\n",
    "对于缺失值比例不是很大的特征都采用算法拟合来填充，用没有缺失的特征属性来预测某些有缺失的特征属性；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前有三类处理方法：\n",
    "\n",
    "1. 用平均值、中值、分位数、众数、随机值等替代。效果一般，因为等于人为增加了噪声。\n",
    "\n",
    "2. 用其他变量做预测模型来算出缺失变量。效果比方法1略好。有一个根本缺陷，如果其他变量和缺失变量无关，则预测的结果无意义。如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。\n",
    "\n",
    "3. 最精确的做法，把变量映射到高维空间。比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。\n",
    "而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。\n",
    "\n",
    "数值型的话，均值和近邻或许是更好的方法。做成哑变量更适合分类、顺序型变量\n",
    "\n",
    "连续变量可以离散化，比如1-10 之间的连续变量可以离散化成10个区间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 实现one hot encode的两种方法：Refer：\n",
    "https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n",
    "\n",
    "- **利用pandas实现one hot  encode:**\n",
    "\n",
    "```\n",
    "#  transform a given column into one hot. Use prefix to have multiple dummies\n",
    ">>> import pandas as pd\n",
    ">>> df = pd.DataFrame({'A': ['a', 'b', 'c'], 'B': ['b', 'a', 'c']})\n",
    ">>> # Get one hot encoding of columns B\n",
    "... \n",
    ">>> df\n",
    "   A  B\n",
    "0  a  b\n",
    "1  b  a\n",
    "2  c  c\n",
    ">>> one_hot = pd.get_dummies(df['B'])\n",
    ">>> # Drop columns B as it is now encoded\n",
    "... \n",
    ">>> df = df.drop('B', axis=1)\n",
    ">>> # Join the encoded df\n",
    "... \n",
    ">>> df = df.join(one_hot)\n",
    ">>> df\n",
    "   A  a  b  c\n",
    "0  a  0  1  0\n",
    "1  b  1  0  0\n",
    "2  c  0  0  1\n",
    "```\n",
    "- **一个定性特征哑编码的demo：**\n",
    "```\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "```\n",
    "\n",
    "- **使用 sklearn进行特征变量哑编码：**\n",
    "```\n",
    ">>> from sklearn.preprocessing import OneHotEncoder\n",
    ">>> enc = OneHotEncoder()\n",
    ">>> enc.fit([[0, 0, 3], [1,1,0], [0,2,1], [1,0,2]])\n",
    "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
    "       handle_unknown='error', n_values='auto', sparse=True)\n",
    ">>> enc.n_values_\n",
    "array([2, 3, 4])\n",
    ">>> enc.feature_indices_\n",
    "array([0, 2, 5, 9])\n",
    ">>> enc.transform([[0,1,1]])\n",
    "<1x9 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 3 stored elements in Compressed Sparse Row format>\n",
    ">>> enc.transform([[0,1,1]]).toarray()\n",
    "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n",
    "```\n",
    "\n",
    "- **一个保存在全局的Label_Binarizer的demo：**\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(all_your_labels_list) # need to be global or remembered to use it later\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return label_binarizer.transform(x)\n",
    "```\n",
    "\n",
    "##### Python pandas: check if any value is NaN in DataFrame\n",
    "```\n",
    "# 查看每一列是否有NaN：\n",
    "df.isnull().any(axis=0)\n",
    "# 查看每一行是否有NaN：\n",
    "df.isnull().any(axis=1)\n",
    "\n",
    "# 查看所有数据中是否有NaN最快的：\n",
    "df.isnull().values.any()\n",
    "\n",
    "# In [2]: df = pd.DataFrame(np.random.randn(1000,1000))\n",
    "\n",
    "In [3]: df[df > 0.9] = pd.np.nan\n",
    "\n",
    "In [4]: %timeit df.isnull().any().any()\n",
    "100 loops, best of 3: 14.7 ms per loop\n",
    "\n",
    "In [5]: %timeit df.isnull().values.sum()\n",
    "100 loops, best of 3: 2.15 ms per loop\n",
    "\n",
    "In [6]: %timeit df.isnull().sum().sum()\n",
    "100 loops, best of 3: 18 ms per loop\n",
    "\n",
    "In [7]: %timeit df.isnull().values.any()\n",
    "1000 loops, best of 3: 948 µs per loop\n",
    "\n",
    "# df.isnull().sum().sum() is a bit slower, but of course, has additional information -- the number of NaNs.\n",
    "```\n",
    "##### **pandas中df.ix, df.loc, df.iloc 的使用场景以及区别：**\n",
    "https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation\n",
    "```\n",
    "# Note: in pandas version 0.20.0 and above, ix is deprecated and the use of loc and iloc is encouraged instead.\n",
    "\n",
    "# First, a recap:\n",
    "  ● loc works on labels in the index.\n",
    "  ● iloc works on the positions in the index (so it only takes integers).\n",
    "  ● ix usually tries to behave like loc but falls back to behaving like iloc if the label is not in the index.\n",
    "\n",
    "# Combining position-based and label-based indexing\n",
    ">>> df = pd.DataFrame(np.nan, \n",
    "                      index=list('abcde'),\n",
    "                      columns=['x','y','z', 8, 9])\n",
    ">>> df\n",
    "    x   y   z   8   9\n",
    "a NaN NaN NaN NaN NaN\n",
    "b NaN NaN NaN NaN NaN\n",
    "c NaN NaN NaN NaN NaN\n",
    "d NaN NaN NaN NaN NaN\n",
    "e NaN NaN NaN NaN NaN\n",
    "\n",
    ">>> df.ix[:'c', :4]\n",
    "    x   y   z   8\n",
    "a NaN NaN NaN NaN\n",
    "b NaN NaN NaN NaN\n",
    "c NaN NaN NaN NaN\n",
    "\n",
    ">>> df.iloc[:df.index.get_loc('c') + 1, :4]\n",
    "    x   y   z   8\n",
    "a NaN NaN NaN NaN\n",
    "b NaN NaN NaN NaN\n",
    "c NaN NaN NaN NaN\n",
    "\n",
    "# get_loc() is an index method meaning \"get the position of the label in this index\".\n",
    "# Note that since slicing with iloc is exclusive of its endpoint, we must add 1 to this value if we want row 'c' as well\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
